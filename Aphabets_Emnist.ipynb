{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aphabets_Emnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mojo46/Machine-Learning-noob/blob/master/Aphabets_Emnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36tAMdIIfDoW",
        "colab_type": "text"
      },
      "source": [
        "# Import Data from kaggle \n",
        "\n",
        "Login to kaggle generate your api key AKA kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akJ8fvsfGVdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDLXrqcNGlU7",
        "colab_type": "code",
        "outputId": "ec21ac00-3336-4553-953f-65ec7e9aab15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 68 Nov 18 12:16 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QITWh1N5GoqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KixiXyagGy7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_-tWp4yt_wB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4b1ce689-0af2-4c03-a6fa-bd2238375347"
      },
      "source": [
        "!kaggle datasets download -d crawford/emnist"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading emnist.zip to /content\n",
            "100% 1.24G/1.24G [00:27<00:00, 50.8MB/s]\n",
            "100% 1.24G/1.24G [00:27<00:00, 48.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F78MVLw4uMVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/emnist.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaRsBsbDfU0m",
        "colab_type": "text"
      },
      "source": [
        "# Data prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkL2xGkSvBn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "80556718-df93-4acb-9773-792a5a36c3df"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3OQXRuzXxa7",
        "colab_type": "text"
      },
      "source": [
        "Read csv into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9nXc9NivseO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emnist_letters_train = pd.read_csv('/content/emnist-letters-train.csv', header=None)\n",
        "emnist_letters_test = pd.read_csv('/content/emnist-letters-test.csv', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47I4S8AOXdy8",
        "colab_type": "text"
      },
      "source": [
        "Convert dataframe to np array and one hot encode the lables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwnU7IjsRXuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(x:pd):\n",
        "  nparr = np.array(x)\n",
        "  data = (nparr[:,1 :])/255\n",
        "  lab = nparr[:,0]\n",
        "  label = np_utils.to_categorical(lab,27,dtype='float32')\n",
        "  return (data , label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRei4x6eSicJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(emnist_letters_train_data1 ,emnist_letters_train_lable1 )=prepare_data(emnist_letters_train)\n",
        "(emnist_letters_test_data1 ,emnist_letters_test_lable1 )=prepare_data(emnist_letters_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c92d1qKQWL4W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67db34aa-b4cd-40ab-e973-f92fedbd0a33"
      },
      "source": [
        "print(emnist_letters_train_data1[0],emnist_letters_train_lable1[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00392157 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.07843137 0.44705882 0.49019608 0.32156863\n",
            " 0.14509804 0.14509804 0.15294118 0.45098039 0.61568627 0.36862745\n",
            " 0.08235294 0.03921569 0.1254902  0.14509804 0.14509804 0.14509804\n",
            " 0.14509804 0.14509804 0.14509804 0.14509804 0.32156863 0.49019608\n",
            " 0.44705882 0.07843137 0.         0.         0.         0.01176471\n",
            " 0.42745098 0.96078431 0.97647059 0.91372549 0.85098039 0.85098039\n",
            " 0.85098039 0.96078431 0.98431373 0.91764706 0.6745098  0.55686275\n",
            " 0.79607843 0.85098039 0.85098039 0.85098039 0.85098039 0.85098039\n",
            " 0.85098039 0.85098039 0.91372549 0.97647059 0.96078431 0.42745098\n",
            " 0.01176471 0.         0.         0.01568627 0.44705882 0.99215686\n",
            " 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
            " 0.99607843 0.99607843 0.98823529 0.98431373 0.99607843 0.99607843\n",
            " 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
            " 0.99607843 0.99607843 0.99607843 0.49803922 0.01568627 0.\n",
            " 0.         0.         0.17647059 0.83921569 0.91372549 0.98431373\n",
            " 0.99607843 0.99607843 0.99607843 0.99607843 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         0.99607843 0.99607843\n",
            " 0.98431373 0.44313725 0.01568627 0.         0.         0.\n",
            " 0.         0.01960784 0.08627451 0.32156863 0.50196078 0.62745098\n",
            " 0.72156863 0.85098039 0.85098039 0.85490196 0.97647059 0.99607843\n",
            " 1.         1.         1.         1.         1.         0.99607843\n",
            " 0.99607843 0.96862745 0.84705882 0.66666667 0.3254902  0.02745098\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00784314 0.01960784 0.0627451  0.09803922 0.14509804\n",
            " 0.15686275 0.19607843 0.89019608 0.99607843 1.         1.\n",
            " 0.99607843 0.99607843 0.98823529 0.96078431 0.8627451  0.62352941\n",
            " 0.14901961 0.08235294 0.01176471 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.03529412 0.30196078 0.54901961\n",
            " 0.96862745 0.99607843 0.99607843 0.99607843 0.99607843 0.95686275\n",
            " 0.69411765 0.44705882 0.19607843 0.07843137 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.01176471\n",
            " 0.13333333 0.68627451 0.96470588 0.99607843 0.99607843 0.99607843\n",
            " 0.99215686 0.86666667 0.45098039 0.18039216 0.02745098 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.01176471 0.13333333 0.32941176 0.68627451 0.98431373\n",
            " 0.99607843 0.99607843 0.99215686 0.91764706 0.79215686 0.35686275\n",
            " 0.01568627 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.03921569 0.30980392\n",
            " 0.8        0.91372549 0.98431373 0.99607843 1.         1.\n",
            " 0.9372549  0.45490196 0.1372549  0.02745098 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.07843137 0.48235294 0.8627451  0.99607843 0.99607843\n",
            " 0.99607843 1.         1.         1.         0.97254902 0.64705882\n",
            " 0.14509804 0.08235294 0.1254902  0.03529412 0.01568627 0.01568627\n",
            " 0.01568627 0.01568627 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.39215686\n",
            " 0.93333333 0.99607843 1.         1.         1.         1.\n",
            " 1.         1.         0.99607843 0.99215686 0.92156863 0.91764706\n",
            " 0.96078431 0.87058824 0.85098039 0.85098039 0.85098039 0.79607843\n",
            " 0.45098039 0.1254902  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.12941176 0.63921569 0.95686275\n",
            " 0.99607843 0.99607843 0.99607843 1.         1.         1.\n",
            " 1.         0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
            " 0.99607843 0.99607843 0.99607843 0.99607843 0.94117647 0.43529412\n",
            " 0.01176471 0.         0.         0.         0.         0.\n",
            " 0.         0.00392157 0.12941176 0.49411765 0.84705882 0.91372549\n",
            " 0.98039216 0.99607843 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 0.99607843 0.99607843 0.91372549 0.32156863 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01960784 0.08627451 0.32156863 0.98431373\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 0.99607843 0.99607843 0.98039216 0.91372549 0.85098039 0.79607843\n",
            " 0.43137255 0.07058824 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.03921569 0.50196078 0.99607843 1.         1.\n",
            " 0.99607843 0.99607843 0.99607843 0.99607843 0.96470588 0.81568627\n",
            " 0.54901961 0.32156863 0.15294118 0.1254902  0.01568627 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.08627451 0.37254902\n",
            " 0.86666667 0.99607843 1.         0.99607843 0.99215686 0.91764706\n",
            " 0.85098039 0.84313725 0.49803922 0.18039216 0.03529412 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.01176471 0.60392157 0.90980392 0.99215686 0.99607843\n",
            " 0.99607843 0.98823529 0.81176471 0.37254902 0.15294118 0.14509804\n",
            " 0.03137255 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00784314 0.30196078\n",
            " 0.96862745 0.99607843 1.         0.99607843 0.94901961 0.51372549\n",
            " 0.03529412 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01568627 0.49019608 0.99607843 0.99607843\n",
            " 0.98823529 0.90980392 0.51372549 0.1254902  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.01568627 0.44313725 0.99215686 0.96470588 0.69411765 0.42745098\n",
            " 0.08627451 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.03137255\n",
            " 0.43529412 0.30196078 0.03137255 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.01176471 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMX4YyX1boNu",
        "colab_type": "text"
      },
      "source": [
        "Simple number to alphabets function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8LWYDgrLdra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alphabet(x : int):\n",
        "  if(x == 1): return 'A'\n",
        "  elif(x==2): return 'B'\n",
        "  elif(x==3): return 'C'\n",
        "  elif(x==4): return 'D'\n",
        "  elif(x==5): return 'E'\n",
        "  elif(x==6): return 'F'\n",
        "  elif(x==7): return 'G'\n",
        "  elif(x==8): return 'H'\n",
        "  elif(x==9): return 'I'\n",
        "  elif(x==10): return 'J'\n",
        "  elif(x==11): return 'K'\n",
        "  elif(x==12): return 'L'\n",
        "  elif(x==13): return 'M'\n",
        "  elif(x==14): return 'N'\n",
        "  elif(x==15): return 'O'\n",
        "  elif(x==16): return 'P'\n",
        "  elif(x==17): return 'Q'\n",
        "  elif(x==18): return 'R'\n",
        "  elif(x==19): return 'S'\n",
        "  elif(x==20): return 'T'\n",
        "  elif(x==21): return 'U'\n",
        "  elif(x==22): return 'V'\n",
        "  elif(x==23): return 'W'\n",
        "  elif(x==24): return 'X'\n",
        "  elif(x==25): return 'Y'\n",
        "  elif(x==26): return 'Z'\n",
        "  else :  return 'nothing'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SohjNbJyX6tT",
        "colab_type": "text"
      },
      "source": [
        "Test images with random index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ollG3Vkg0vJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 10654\n",
        "plt.imshow(emnist_letters_test_data1[index].reshape(28,28))\n",
        "print(alphabet(emnist_letters_test_lable1[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J37uhcdFwykI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /content/emnist-letters-mapping.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJLXp50MmB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(alphabet(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVjPjdk4MoOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b4e7d4df-0f0d-4361-d79a-e7bf3beaa16c"
      },
      "source": [
        "print(emnist_letters_train_data1.shape,emnist_letters_train_lable1[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(88800, 784) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zOYmGioOeva",
        "colab_type": "text"
      },
      "source": [
        "# Model Archi and Complie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsjgvmUN_3Gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, input_dim=784))\n",
        "# An \"activation\" is just a non-linear function applied to the output\n",
        "# of the layer above. Here, with a \"rectified linear unit\",\n",
        "# we clamp all values below 0 to 0.\n",
        "model.add(Activation('relu'))\n",
        "# Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(27))\n",
        "# This special \"softmax\" activation among other things,\n",
        "# ensures the output is a valid probaility distribution, that is\n",
        "# that its values are all non-negative and sum to 1.\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVy_X8PyOqTh",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JacoV0WVOroo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8f2bad6f-4ce2-4c11-83b6-afc25c4d3506"
      },
      "source": [
        "hist = model.fit(emnist_letters_train_data1, emnist_letters_train_lable1, batch_size=None, nb_epoch=100, validation_data=(emnist_letters_test_data1, emnist_letters_test_lable1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88800/88800 [==============================] - 22s 249us/step - loss: 0.5818 - acc: 0.8157 - val_loss: 0.4028 - val_acc: 0.8664\n",
            "Epoch 2/100\n",
            "88800/88800 [==============================] - 17s 193us/step - loss: 0.3302 - acc: 0.8915 - val_loss: 0.3706 - val_acc: 0.8802\n",
            "Epoch 3/100\n",
            "88800/88800 [==============================] - 17s 195us/step - loss: 0.2674 - acc: 0.9101 - val_loss: 0.3908 - val_acc: 0.8776\n",
            "Epoch 4/100\n",
            "88800/88800 [==============================] - 17s 195us/step - loss: 0.2314 - acc: 0.9205 - val_loss: 0.3658 - val_acc: 0.8857\n",
            "Epoch 5/100\n",
            "88800/88800 [==============================] - 17s 194us/step - loss: 0.2057 - acc: 0.9270 - val_loss: 0.3693 - val_acc: 0.8911\n",
            "Epoch 6/100\n",
            "88800/88800 [==============================] - 17s 196us/step - loss: 0.1845 - acc: 0.9334 - val_loss: 0.3706 - val_acc: 0.8908\n",
            "Epoch 7/100\n",
            "88800/88800 [==============================] - 17s 192us/step - loss: 0.1734 - acc: 0.9368 - val_loss: 0.4062 - val_acc: 0.8880\n",
            "Epoch 8/100\n",
            "88800/88800 [==============================] - 17s 190us/step - loss: 0.1622 - acc: 0.9415 - val_loss: 0.3903 - val_acc: 0.8918\n",
            "Epoch 9/100\n",
            "88800/88800 [==============================] - 17s 195us/step - loss: 0.1494 - acc: 0.9453 - val_loss: 0.4600 - val_acc: 0.8864\n",
            "Epoch 10/100\n",
            "88800/88800 [==============================] - 17s 191us/step - loss: 0.1449 - acc: 0.9476 - val_loss: 0.4250 - val_acc: 0.8901\n",
            "Epoch 11/100\n",
            "88800/88800 [==============================] - 17s 194us/step - loss: 0.1403 - acc: 0.9491 - val_loss: 0.4500 - val_acc: 0.8952\n",
            "Epoch 12/100\n",
            "88800/88800 [==============================] - 17s 196us/step - loss: 0.1352 - acc: 0.9512 - val_loss: 0.4703 - val_acc: 0.8932\n",
            "Epoch 13/100\n",
            "88800/88800 [==============================] - 17s 194us/step - loss: 0.1363 - acc: 0.9516 - val_loss: 0.4686 - val_acc: 0.8934\n",
            "Epoch 14/100\n",
            "88800/88800 [==============================] - 17s 195us/step - loss: 0.1303 - acc: 0.9536 - val_loss: 0.5050 - val_acc: 0.8914\n",
            "Epoch 15/100\n",
            "88800/88800 [==============================] - 17s 191us/step - loss: 0.1280 - acc: 0.9555 - val_loss: 0.4994 - val_acc: 0.8960\n",
            "Epoch 16/100\n",
            "88800/88800 [==============================] - 17s 196us/step - loss: 0.1278 - acc: 0.9551 - val_loss: 0.5191 - val_acc: 0.8957\n",
            "Epoch 17/100\n",
            "88800/88800 [==============================] - 17s 192us/step - loss: 0.1247 - acc: 0.9565 - val_loss: 0.5316 - val_acc: 0.8927\n",
            "Epoch 18/100\n",
            "88800/88800 [==============================] - 17s 191us/step - loss: 0.1211 - acc: 0.9573 - val_loss: 0.5265 - val_acc: 0.8951\n",
            "Epoch 19/100\n",
            "88800/88800 [==============================] - 17s 193us/step - loss: 0.1178 - acc: 0.9588 - val_loss: 0.5723 - val_acc: 0.8954\n",
            "Epoch 20/100\n",
            "88800/88800 [==============================] - 17s 195us/step - loss: 0.1182 - acc: 0.9595 - val_loss: 0.5581 - val_acc: 0.8943\n",
            "Epoch 21/100\n",
            "88800/88800 [==============================] - 17s 191us/step - loss: 0.1194 - acc: 0.9592 - val_loss: 0.5590 - val_acc: 0.8941\n",
            "Epoch 22/100\n",
            "88800/88800 [==============================] - 17s 192us/step - loss: 0.1190 - acc: 0.9595 - val_loss: 0.5605 - val_acc: 0.8926\n",
            "Epoch 23/100\n",
            "88800/88800 [==============================] - 17s 197us/step - loss: 0.1187 - acc: 0.9599 - val_loss: 0.6435 - val_acc: 0.8880\n",
            "Epoch 24/100\n",
            "88800/88800 [==============================] - 17s 194us/step - loss: 0.1178 - acc: 0.9609 - val_loss: 0.6227 - val_acc: 0.8960\n",
            "Epoch 25/100\n",
            "88800/88800 [==============================] - 17s 190us/step - loss: 0.1141 - acc: 0.9619 - val_loss: 0.5939 - val_acc: 0.8947\n",
            "Epoch 26/100\n",
            "88800/88800 [==============================] - 17s 191us/step - loss: 0.1196 - acc: 0.9618 - val_loss: 0.6053 - val_acc: 0.8908\n",
            "Epoch 27/100\n",
            "88800/88800 [==============================] - 17s 189us/step - loss: 0.1172 - acc: 0.9618 - val_loss: 0.6235 - val_acc: 0.8963\n",
            "Epoch 28/100\n",
            "88800/88800 [==============================] - 16s 185us/step - loss: 0.1161 - acc: 0.9624 - val_loss: 0.6461 - val_acc: 0.8917\n",
            "Epoch 29/100\n",
            "88800/88800 [==============================] - 17s 187us/step - loss: 0.1185 - acc: 0.9628 - val_loss: 0.6794 - val_acc: 0.8923\n",
            "Epoch 30/100\n",
            "88800/88800 [==============================] - 17s 189us/step - loss: 0.1224 - acc: 0.9617 - val_loss: 0.6561 - val_acc: 0.8954\n",
            "Epoch 31/100\n",
            "88800/88800 [==============================] - 17s 188us/step - loss: 0.1167 - acc: 0.9639 - val_loss: 0.7165 - val_acc: 0.8914\n",
            "Epoch 32/100\n",
            "88800/88800 [==============================] - 17s 188us/step - loss: 0.1156 - acc: 0.9636 - val_loss: 0.6690 - val_acc: 0.8951\n",
            "Epoch 33/100\n",
            "88800/88800 [==============================] - 17s 188us/step - loss: 0.1202 - acc: 0.9644 - val_loss: 0.7058 - val_acc: 0.8903\n",
            "Epoch 34/100\n",
            "88800/88800 [==============================] - 17s 188us/step - loss: 0.1203 - acc: 0.9628 - val_loss: 0.7064 - val_acc: 0.8904\n",
            "Epoch 35/100\n",
            "88800/88800 [==============================] - 17s 189us/step - loss: 0.1115 - acc: 0.9656 - val_loss: 0.7433 - val_acc: 0.8885\n",
            "Epoch 36/100\n",
            "88800/88800 [==============================] - 17s 187us/step - loss: 0.1209 - acc: 0.9637 - val_loss: 0.7837 - val_acc: 0.8886\n",
            "Epoch 37/100\n",
            "88800/88800 [==============================] - 17s 187us/step - loss: 0.1169 - acc: 0.9654 - val_loss: 0.7368 - val_acc: 0.8945\n",
            "Epoch 38/100\n",
            "88800/88800 [==============================] - 17s 186us/step - loss: 0.1223 - acc: 0.9647 - val_loss: 0.7446 - val_acc: 0.8911\n",
            "Epoch 39/100\n",
            "88800/88800 [==============================] - 16s 181us/step - loss: 0.1158 - acc: 0.9659 - val_loss: 0.7780 - val_acc: 0.8917\n",
            "Epoch 40/100\n",
            "10688/88800 [==>...........................] - ETA: 13s - loss: 0.1110 - acc: 0.9668"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVt2MilPbE4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoQUAVJMflV_",
        "colab_type": "text"
      },
      "source": [
        "# Display results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiSKGn2jVhkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(14,3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.plot(hist.history['loss'], color='b', label='Training Loss')\n",
        "plt.plot(hist.history['val_loss'], color='r', label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.plot(hist.history['acc'], color='b', label='Training Accuracy')\n",
        "plt.plot(hist.history['val_acc'], color='r', label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}